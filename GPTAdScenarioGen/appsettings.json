{
  "Kestrel": {
    "EndpointDefaults": {
      "Protocols": "Http1AndHttp2"
    },
    "EndPoints": {
      "Http": {
        "Url": "http://0.0.0.0"
      },
      "Https": {
        "Url": "https://0.0.0.0"
      },
      "HttpsApi": {
        "Url": "https://0.0.0.0:5000"
      }
    },
    "Certificate": {
      "Path": "C:\\certificate.pfx",
      "Password": "righteousness"
    }
  },
  "AllowedHosts": "*",
  "QueryTemplatePath": "./query.txt", //Путь к файлу, содержащему шаблон запроса для ChatGPT. Имеет приоритет выше, чем QueryTemplate.
  "QueryFrontendPath": "./query_frontend.txt", //Путь к файлу, содержащему шаблон запроса, который передаётся фронтенду. Количество ячеек шаблона должно совпадать с шаблоном для ChatGPT
  //"QueryTemplate": "Give me a short story about {0}", //Базовый шаблон запроса. Имеет приоритет ниже, чем QueryTemplatePath.
  //Если не указано ни QueryTemplatePath, ни QueryTemplate, запрос отправляется как есть, без вставления в шаблоны.
  //Шаблоны должны выглядеть так:
  //Расскажи историю про {0} и то как у {0} выросли {1}
  //Методы, принимающие массивы строк, поместят первую строку вместо {0}, а вторую вместо {1}
  //Например, если будет передан массив ["паука", "уши"], то лингвистической модели будет передан следующий запрос:
  //Расскажи историю про паука и то как у паука выросли уши
  //Дополнительно про форматирование строк: https://www.c-sharpcorner.com/UploadFile/mahesh/format-string-in-C-Sharp/
  "MaxResponseTokens": 500, //Максимальное количество жетонов, которые вернёт лингвистическая модель в ответ на запрос
  "MaxRequestLength": 100, //Максимальная длина итогового запроса
  "MaxRequestsPerHour": 50, //Максимальное количество запросов в час (количество текущих запросов сбрасывается каждый час)
  "Temperature": 0.8, //Температура модели. Мин. 0, макс. 1. Влияет на "воображение" модели - чем выше значение, тем больше шансов получить другой результат на одинаковый запрос
  "ApiKeyPath": "./key.txt", //Путь к ключу от API. 
  "Model": "gpt-3.5-turbo", //Код используемой модели. Стандартные поддерживаемые модели: можно найти здесь: https://github.com/betalgo/openai/wiki/Models
  "DebugResponse": false,
  "ResponseThrottling": 0,
  "ResponseMinChunkSize": 0
}
